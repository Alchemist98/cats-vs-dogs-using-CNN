{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPool2D,BatchNormalization,Dropout,Flatten,Dense\nfrom keras.preprocessing.image import ImageDataGenerator","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_data = ImageDataGenerator()\ntrain_data = tr_data.flow_from_directory(directory=\"../input/cats-and-dogs-filtered/cats_and_dogs_filtered/train\",target_size=(IMAGE_WIDTH,IMAGE_HEIGHT))","execution_count":64,"outputs":[{"output_type":"stream","text":"Found 2000 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"te_data = ImageDataGenerator()\ntest_data = te_data.flow_from_directory(directory=\"../input/cats-and-dogs-filtered/cats_and_dogs_filtered/validation/\",\n                                       target_size=(IMAGE_WIDTH,IMAGE_HEIGHT))","execution_count":65,"outputs":[{"output_type":"stream","text":"Found 1000 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model =Sequential()","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1st layer\nmodel.add(Conv2D(64,kernel_size=(3,3),padding='same',activation='relu',input_shape=(224,224,3)))\nmodel.add(Conv2D(64,kernel_size=(3,3),padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n#model.add(BatchNormalization())","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2nd layer\nmodel.add(Conv2D(128,kernel_size=(3,3),padding='same',activation='relu'))\nmodel.add(Conv2D(128,kernel_size=(3,3),padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n#model.add(BatchNormalization())","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3rd layer\nmodel.add(Conv2D(256,kernel_size=(3,3),padding='same',activation='relu'))\nmodel.add(Conv2D(256,kernel_size=(3,3),padding='same',activation='relu'))\nmodel.add(Conv2D(256,kernel_size=(3,3),padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n#model.add(BatchNormalization())","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4th layer\nmodel.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\nmodel.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\nmodel.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n#model.add(BatchNormalization())","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#5th layer\nmodel.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\nmodel.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\nmodel.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n#model.add(BatchNormalization())","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Flatten())","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(4096,activation='relu'))","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(4096,activation='relu'))","execution_count":74,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(2,activation='sigmoid'))","execution_count":75,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import Adam\nopt = Adam(lr=0.001)\nmodel.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":77,"outputs":[{"output_type":"stream","text":"Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_40 (Conv2D)           (None, 224, 224, 64)      1792      \n_________________________________________________________________\nconv2d_41 (Conv2D)           (None, 224, 224, 64)      36928     \n_________________________________________________________________\nmax_pooling2d_15 (MaxPooling (None, 112, 112, 64)      0         \n_________________________________________________________________\nconv2d_42 (Conv2D)           (None, 112, 112, 128)     73856     \n_________________________________________________________________\nconv2d_43 (Conv2D)           (None, 112, 112, 128)     147584    \n_________________________________________________________________\nmax_pooling2d_16 (MaxPooling (None, 56, 56, 128)       0         \n_________________________________________________________________\nconv2d_44 (Conv2D)           (None, 56, 56, 256)       295168    \n_________________________________________________________________\nconv2d_45 (Conv2D)           (None, 56, 56, 256)       590080    \n_________________________________________________________________\nconv2d_46 (Conv2D)           (None, 56, 56, 256)       590080    \n_________________________________________________________________\nmax_pooling2d_17 (MaxPooling (None, 28, 28, 256)       0         \n_________________________________________________________________\nconv2d_47 (Conv2D)           (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nconv2d_48 (Conv2D)           (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nconv2d_49 (Conv2D)           (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nmax_pooling2d_18 (MaxPooling (None, 14, 14, 512)       0         \n_________________________________________________________________\nconv2d_50 (Conv2D)           (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nconv2d_51 (Conv2D)           (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nconv2d_52 (Conv2D)           (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nmax_pooling2d_19 (MaxPooling (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 25088)             0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 4096)              102764544 \n_________________________________________________________________\ndense_8 (Dense)              (None, 4096)              16781312  \n_________________________________________________________________\ndense_9 (Dense)              (None, 2)                 8194      \n=================================================================\nTotal params: 134,268,738\nTrainable params: 134,268,738\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit_generator(steps_per_epoch=100,generator=train_data,\n                           validation_data= test_data, validation_steps=10,\n                           epochs=1)","execution_count":79,"outputs":[{"output_type":"stream","text":"Epoch 1/1\n100/100 [==============================] - 2531s 25s/step - loss: 7.9987 - accuracy: 0.5035 - val_loss: 7.0517 - val_accuracy: 0.5000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import image\nimg = image.load_img(\"../input/cats-and-dogs-filtered/cats_and_dogs_filtered/validation/cats/cat.2015.jpg\",target_size=(224,224))\nimg = np.asarray(img)\nplt.imshow(img)\nimg = np.expand_dims(img, axis=0)\noutput = model.predict(img)\nif output[0][0] > output[0][1]:\n    print(\"cat\")\nelse:\n    print('dog')","execution_count":86,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/cats-and-dogs-filtered/cats_and_dogs_filtered/validation/cats/dog.2015.jpg'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-86-58e6ad12ac97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/cats-and-dogs-filtered/cats_and_dogs_filtered/validation/cats/dog.2015.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/cats-and-dogs-filtered/cats_and_dogs_filtered/validation/cats/dog.2015.jpg'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}